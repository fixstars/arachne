{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run onnx-tf from Arachne\n",
    "\n",
    "[onnx-tf](https://github.com/onnx/onnx-tensorflow) is a ONNX to Tensorflow converter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a Model\n",
    "\n",
    "First, we have to prepare a model to be used in this tutorial.\n",
    "Here, we will use a pre-trained model of the ResNet-18 from onnx models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-28 08:29:59--  https://github.com/onnx/models/blob/main/vision/classification/resnet/model/resnet18-v1-7.onnx?raw=true\n",
      "Resolving github.com (github.com)... 52.192.72.89\n",
      "Connecting to github.com (github.com)|52.192.72.89|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/onnx/models/raw/main/vision/classification/resnet/model/resnet18-v1-7.onnx [following]\n",
      "--2022-03-28 08:29:59--  https://github.com/onnx/models/raw/main/vision/classification/resnet/model/resnet18-v1-7.onnx\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://media.githubusercontent.com/media/onnx/models/main/vision/classification/resnet/model/resnet18-v1-7.onnx [following]\n",
      "--2022-03-28 08:29:59--  https://media.githubusercontent.com/media/onnx/models/main/vision/classification/resnet/model/resnet18-v1-7.onnx\n",
      "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 46820735 (45M) [application/octet-stream]\n",
      "Saving to: ‘resnet18.onnx’\n",
      "\n",
      "resnet18.onnx       100%[===================>]  44.65M  62.5MB/s    in 0.7s    \n",
      "\n",
      "2022-03-28 08:30:04 (62.5 MB/s) - ‘resnet18.onnx’ saved [46820735/46820735]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/onnx/models/blob/main/vision/classification/resnet/model/resnet18-v1-7.onnx?raw=true -O resnet18.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run onnx-tf from Arachne\n",
    "\n",
    "Now, let's convert the model with the onnx-tf by Arachne.\n",
    "To use the tool, we have to specify `+tools=onnx_tf` to `arachne.driver.cli`.\n",
    "Available options can be seen by adding `--help`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cli is powered by Hydra.\n",
      "\n",
      "== Configuration groups ==\n",
      "Compose your configuration from those groups (group=option)\n",
      "\n",
      "tools: onnx_simplifier, onnx_tf, openvino2tf, openvino_mo, tflite_converter, tftrt, torch2onnx, torch2trt, tvm\n",
      "tvm_target: dgx-1, dgx-s, jetson-nano, jetson-xavier-nx, rasp4b64\n",
      "\n",
      "\n",
      "== Config ==\n",
      "Override anything in the config (foo.bar=value)\n",
      "\n",
      "input: ???\n",
      "input_spec: null\n",
      "output: ???\n",
      "tools:\n",
      "  onnx_tf:\n",
      "    cli_args: null\n",
      "\n",
      "\n",
      "Powered by Hydra (https://hydra.cc)\n",
      "Use --hydra-help to view Hydra specific help\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python -m arachne.driver.cli +tools=onnx_tf --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert a model by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-28 08:30:17,285 - onnx-tf - INFO - Start converting onnx pb to tf pb:\n",
      "2022-03-28 08:30:17.507379: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-28 08:30:20.377143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 373 MB memory:  -> device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "2022-03-28 08:30:20.379427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30546 MB memory:  -> device: 1, name: Tesla V100-DGXS-32GB, pci bus id: 0000:08:00.0, compute capability: 7.0\n",
      "2022-03-28 08:30:20.380594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 2581 MB memory:  -> device: 2, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0e:00.0, compute capability: 7.0\n",
      "2022-03-28 08:30:20.382746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30546 MB memory:  -> device: 3, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0f:00.0, compute capability: 7.0\n",
      "2022-03-28 08:30:35.519800: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "2022-03-28 08:30:37,145 - onnx-tf - INFO - Converting completes successfully.\n",
      "INFO:onnx-tf:Converting completes successfully.\n",
      "2022-03-28 08:30:38.319951: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-28 08:30:41.041932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 373 MB memory:  -> device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "2022-03-28 08:30:41.044498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30546 MB memory:  -> device: 1, name: Tesla V100-DGXS-32GB, pci bus id: 0000:08:00.0, compute capability: 7.0\n",
      "2022-03-28 08:30:41.045667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 2581 MB memory:  -> device: 2, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0e:00.0, compute capability: 7.0\n",
      "2022-03-28 08:30:41.047874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30546 MB memory:  -> device: 3, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0f:00.0, compute capability: 7.0\n",
      "model_0-saved_model/\n",
      "model_0-saved_model/saved_model.pb\n",
      "model_0-saved_model/variables/\n",
      "model_0-saved_model/variables/variables.data-00000-of-00001\n",
      "model_0-saved_model/variables/variables.index\n",
      "model_0-saved_model/assets/\n",
      "env.yaml\n"
     ]
    }
   ],
   "source": [
    "!python -m arachne.driver.cli +tools=onnx_tf model_file=./resnet18.onnx output_path=./output.tar\n",
    "!tar xvf ./output.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run onnx-tf from Arachne Python Interface\n",
    "\n",
    "The following code shows an example of using the tool from Arachne Python interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arachne.utils.model_utils import save_model, init_from_file\n",
    "from arachne.tools.onnx_tf import ONNXTf, ONNXTfConfig\n",
    "\n",
    "model_file_path = \"./resnet18.onnx\"\n",
    "input = init_from_file(model_file_path)\n",
    "\n",
    "cfg = ONNXTfConfig()\n",
    "\n",
    "output = ONNXTf.run(input, cfg)\n",
    "\n",
    "save_model(model=output, output_path=\"./output.tar\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fafabb367c689809e9098d9e06eeddad4dedf0748e439d9c922f0a4231fa874d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
